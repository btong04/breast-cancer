{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b0de4470-4881-4507-929e-4e92190af870",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2020-11-23T22:59:36.557347Z",
     "iopub.status.busy": "2020-11-23T22:59:36.556307Z",
     "iopub.status.idle": "2020-11-23T22:59:42.447973Z",
     "shell.execute_reply": "2020-11-23T22:59:42.447973Z",
     "shell.execute_reply.started": "2020-11-23T22:59:36.557347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xboost version:  1.3.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from functools import reduce\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Import other utils:\n",
    "notebook_path = os.getcwd()\n",
    "root_path = os.path.split(notebook_path)[0]\n",
    "sys.path.append(root_path)\n",
    "import parsing_utils as psu\n",
    "\n",
    "import pyspark\n",
    "\n",
    "\n",
    "# Create SparkContext and submit custom jars:\n",
    "extra_jars_dir = r'C:\\spark\\extra_jars'\n",
    "use_jars = ['xgboost4j-spark_3.0-1.2.0-0.1.0.jar', 'xgboost4j_3.0-1.2.0-0.1.0.jar']\n",
    "use_jars = [os.path.join(extra_jars_dir, jj) for jj in use_jars]\n",
    "spark_jars = reduce(lambda x,y: x+','+y, use_jars)\n",
    "extraClassPath_jars = reduce(lambda x,y: x+':'+y, use_jars)\n",
    "\n",
    "# pyspark has issues with Java 11 and arrow. Set \"-Dio.netty.tryReflectionSetAccessible=true\"\n",
    "# https://stackoverflow.com/questions/62109276/errorjava-lang-unsupportedoperationexception-for-pyspark-pandas-udf-documenta\n",
    "\n",
    "\n",
    "# Create SparkContext (i.e., spark local cluster by default if spark.master not set):\n",
    "config = pyspark.SparkConf().setAll([\n",
    "    ('spark.sql.execution.arrow.pyspark.enabled','true'),\n",
    "    ('spark.driver.memory','4G'), \n",
    "    ('spark.executor.memory','6G'), \n",
    "    ('spark.jars',spark_jars),\n",
    "    ('spark.driver.extraClassPath',extraClassPath_jars), \n",
    "    ('spark.executor.extraClassPath',extraClassPath_jars),\n",
    "    ('spark.driver.extraJavaOptions','-Dio.netty.tryReflectionSetAccessible=true'),\n",
    "    ('spark.executor.extraJavaOptions','-Dio.netty.tryReflectionSetAccessible=true')\n",
    "])\n",
    "sc = pyspark.SparkContext(master='local[4]', conf=config)\n",
    "sc.addPyFile(use_jars[0])\n",
    "# Use sc.stop() to kill SparkContext.\n",
    "\n",
    "# Create SparkSession from existing SparkContext:\n",
    "spark = pyspark.sql.SparkSession(sc)\n",
    "\n",
    "import ml.dmlc.xgboost4j.scala.spark as xgb\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, OneHotEncoderModel, VectorAssembler\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.linalg import SparseVector, DenseVector, Vectors, VectorUDT\n",
    "\n",
    "\n",
    "print('xboost version: ', xgb.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "608b77de-6508-42c6-9107-a25aadc1e402",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2020-11-23T22:59:42.449942Z",
     "iopub.status.busy": "2020-11-23T22:59:42.448974Z",
     "iopub.status.idle": "2020-11-23T23:00:06.923687Z",
     "shell.execute_reply": "2020-11-23T23:00:06.923594Z",
     "shell.execute_reply.started": "2020-11-23T22:59:42.449942Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binary columns detected: ['gender', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling', 'Churn']\n",
      "Numeric columns: ['customerID', 'SeniorCitizen', 'tenure', 'MonthlyCharges', 'TotalCharges']\n",
      "\n",
      "Estimated number of columns after encoding: 41\n"
     ]
    }
   ],
   "source": [
    "# Get column names from single file scan:\n",
    "# data_dir = '/dbfs/FileStore/tables/telco-data' # Databricks\n",
    "data_dir = 'C:/data/telco_dat'\n",
    "# tmp_df = pd.read_csv(data_dir+'/csv/perturb_dat_p0000.csv', nrows=1)\n",
    "tmp_df = pd.read_csv(data_dir+'/csv/perturb_dat_p0000.csv')\n",
    "\n",
    "\n",
    "all_cols = list(tmp_df.columns)\n",
    "\n",
    "# Use spark and one hot encoder to create numeric dmatrix for xgboost.\n",
    "# More details here: https://stackoverflow.com/questions/32982425/encode-and-assemble-multiple-features-in-pyspark\n",
    "# Categorical string columns:\n",
    "cat_str_cols = list(tmp_df.select_dtypes('object').columns)\n",
    "\n",
    "# Categorical numeric columns:\n",
    "cat_num_cols = ['SeniorCitizen']\n",
    "\n",
    "# Numeric columns:\n",
    "num_cols = list(tmp_df.select_dtypes('number').columns)\n",
    "\n",
    "# Ignore columns:\n",
    "ignore_cols = ['customerID']\n",
    "\n",
    "# Determine unique values per column. Used to determine encoding strategy. May require fillna to be invoked.\n",
    "unique_cnt = tmp_df.nunique(axis=0)\n",
    "binary_cols = list(unique_cnt[unique_cnt==2].index)\n",
    "\n",
    "# Remove binary numeric columns:\n",
    "binary_cols = [cc for cc in binary_cols if cc not in num_cols]\n",
    "\n",
    "# One hot encode only non-binary, categorical columns:\n",
    "ohe_cols = list(set(cat_str_cols) - set(binary_cols))\n",
    "\n",
    "print('Binary columns detected:', binary_cols)\n",
    "print('Numeric columns:', num_cols)\n",
    "\n",
    "# TODO: low cardinality numeric cols. These may require one-hot-encoding.\n",
    "\n",
    "# Estimate total number of columns after encoding data:\n",
    "ohe_col_cnt = np.sum(unique_cnt[(unique_cnt > 2) & (unique_cnt < 5)].values)\n",
    "est_col_total = ohe_col_cnt + len(binary_cols) + len(num_cols) - len(ignore_cols)\n",
    "\n",
    "print()\n",
    "print('Estimated number of columns after encoding:', est_col_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3b12be4f-58a7-45a3-a5ce-c909910f1cac",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2020-11-23T23:00:06.927643Z",
     "iopub.status.busy": "2020-11-23T23:00:06.927643Z",
     "iopub.status.idle": "2020-11-23T23:00:06.939644Z",
     "shell.execute_reply": "2020-11-23T23:00:06.938601Z",
     "shell.execute_reply.started": "2020-11-23T23:00:06.927643Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(customerID,FloatType,true),StructField(gender,StringType,true),StructField(SeniorCitizen,FloatType,true),StructField(Partner,StringType,true),StructField(Dependents,StringType,true),StructField(tenure,FloatType,true),StructField(PhoneService,StringType,true),StructField(MultipleLines,StringType,true),StructField(InternetService,StringType,true),StructField(OnlineSecurity,StringType,true),StructField(OnlineBackup,StringType,true),StructField(DeviceProtection,StringType,true),StructField(TechSupport,StringType,true),StructField(StreamingTV,StringType,true),StructField(StreamingMovies,StringType,true),StructField(Contract,StringType,true),StructField(PaperlessBilling,StringType,true),StructField(PaymentMethod,StringType,true),StructField(MonthlyCharges,FloatType,true),StructField(TotalCharges,FloatType,true),StructField(Churn,StringType,true)))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most columns are string. Use exceptions for other dtypes.\n",
    "exception_col_dtype = {cc: FloatType() for cc in num_cols}\n",
    "\n",
    "# # Update dictionary for numeric categorical columns:\n",
    "# exception_col_dtype.update({cc: StringType() for cc in cat_num_cols})\n",
    "\n",
    "schema_lit = []\n",
    "for cc in all_cols:\n",
    "    if cc in exception_col_dtype.keys():\n",
    "        dtype = exception_col_dtype[cc]\n",
    "    else:\n",
    "        dtype = StringType()\n",
    "    \n",
    "    schema_lit.append(StructField(cc, dtype))\n",
    "    \n",
    "# For schema generation, need to have same number of columns in python and spark. \n",
    "# Don't apply the ignore_cols until after data loaded into spark.\n",
    "schema_lit = StructType(schema_lit)\n",
    "\n",
    "schema_lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c5c9f96f-709f-4419-88a9-b44601e9d3cc",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.execute_input": "2020-11-23T23:00:06.941602Z",
     "iopub.status.busy": "2020-11-23T23:00:06.941602Z",
     "iopub.status.idle": "2020-11-23T23:01:41.817507Z",
     "shell.execute_reply": "2020-11-23T23:01:41.816507Z",
     "shell.execute_reply.started": "2020-11-23T23:00:06.941602Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load and apply schema to csv data:\n",
    "data_format = 'csv'\n",
    "# data_format = 'parquet'\n",
    "\n",
    "# Databricks environment doesn't require \"dbfs/\" pre-appended.\n",
    "# data_loc = '/FileStore/tables/telco-data/csv'\n",
    "# data_loc = '/FileStore/tables/telco-data/csv/perturb_dat_p0000.csv'\n",
    "# data_loc = '/FileStore/tables/telco-data/WA_Fn_UseC__Telco_Customer_Churn_mod.csv'\n",
    "data_loc = data_dir + '/'+data_format+'/perturb_dat_p0000.'+data_format\n",
    "\n",
    "if data_format == 'csv':\n",
    "    train_data = spark.read.schema(schema_lit).option('header', True).csv(data_loc)\n",
    "elif data_format == 'parquet':\n",
    "    train_data = spark.read.parquet(data_loc)\n",
    "\n",
    "train_data = train_data.drop(*ignore_cols)\n",
    "\n",
    "# Adapted from: https://stackoverflow.com/questions/36942233/apply-stringindexer-to-several-columns-in-a-pyspark-dataframe\n",
    "# First pass, convert categorical columns to numeric using StringIndexer:\n",
    "indexers = [StringIndexer(inputCol=cc, outputCol=cc+'_index').fit(train_data) for cc in cat_str_cols]\n",
    "\n",
    "# TODO: set to overwrite existing field for better mem management.\n",
    "\n",
    "# Apply oneHotEncoder. Change the dropLast option to False to correspond with pandas get_dummies() default behavior.\n",
    "encoder = OneHotEncoder(inputCols=[cc+'_index' for cc in ohe_cols],\n",
    "                        outputCols=[cc+'_enc' for cc in ohe_cols],\n",
    "                        dropLast=False\n",
    "                       )\n",
    "\n",
    "pipeline = Pipeline(stages=indexers + [encoder])\n",
    "train_data_enc = pipeline.fit(train_data).transform(train_data)\n",
    "\n",
    "# Encode sparse to dense format for xgboost4j to recognize:\n",
    "sparse_to_vector_udf = F.udf(lambda vs: Vectors.dense([float(ii) for ii in vs]), VectorUDT())\n",
    "\n",
    "# Assembler:\n",
    "def vectorize(data_frame):\n",
    "    to_floats = [ col(x.name).cast(FloatType()) for x in data_frame.schema ]\n",
    "    return (VectorAssembler()\n",
    "        .setInputCols(use_cols)\n",
    "        .setOutputCol('features')\n",
    "#         .setHandleInvalid('keep') # How to handle NaN's\n",
    "        .transform(data_frame)\n",
    "        .withColumn('features', sparse_to_vector_udf('features')) # Expand sparse to dense vector format\n",
    "        .select(col('features'), col(label)))\n",
    "\n",
    "# Assembler stage. Make sure ignore_cols are removed and label/target column included.\n",
    "use_cols = [cc+'_enc' for cc in ohe_cols] + [cc+'_index' for cc in binary_cols] + num_cols\n",
    "use_cols = [cc for cc in use_cols if cc not in ignore_cols]\n",
    "\n",
    "label = 'Churn_index' # Needs to be numeric encoded (i.e., with StringIndexer)\n",
    "\n",
    "# Generate the 2 column format expected by cpu version of xgboost:\n",
    "train_data_enc_vec = vectorize(train_data_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T23:01:41.818508Z",
     "iopub.status.busy": "2020-11-23T23:01:41.818508Z",
     "iopub.status.idle": "2020-11-23T23:01:43.832226Z",
     "shell.execute_reply": "2020-11-23T23:01:43.831241Z",
     "shell.execute_reply.started": "2020-11-23T23:01:41.818508Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+\n",
      "|    StreamingMovies|StreamingMovies_enc|\n",
      "+-------------------+-------------------+\n",
      "|                 No|      (3,[0],[1.0])|\n",
      "|                 No|      (3,[0],[1.0])|\n",
      "|                 No|      (3,[0],[1.0])|\n",
      "|                 No|      (3,[0],[1.0])|\n",
      "|                 No|      (3,[0],[1.0])|\n",
      "|                Yes|      (3,[1],[1.0])|\n",
      "|                 No|      (3,[0],[1.0])|\n",
      "|                 No|      (3,[0],[1.0])|\n",
      "|                Yes|      (3,[1],[1.0])|\n",
      "|                 No|      (3,[0],[1.0])|\n",
      "|                 No|      (3,[0],[1.0])|\n",
      "|No internet service|      (3,[2],[1.0])|\n",
      "|                Yes|      (3,[1],[1.0])|\n",
      "|                Yes|      (3,[1],[1.0])|\n",
      "|                Yes|      (3,[1],[1.0])|\n",
      "+-------------------+-------------------+\n",
      "only showing top 15 rows\n",
      "\n",
      "\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+\n",
      "|features                                                                                                                                                                                          |Churn_index|\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+\n",
      "|[0.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,1.0,0.0,0.0,1.0,1.0,0.0,1.0,0.0,0.0,0.0,1.0,34.70000076293945,34.70000076293945] |0.0        |\n",
      "|[0.0,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,35.0,53.470001220703125,1810.75]         |0.0        |\n",
      "|[0.0,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,2.0,51.0099983215332,102.01000213623047] |1.0        |\n",
      "|[0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,50.0,37.7599983215332,1830.3900146484375]|0.0        |\n",
      "|[1.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,2.0,76.81999969482422,153.63999938964844]|1.0        |\n",
      "+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show data format:\n",
    "# Sample encoding from sparkML:\n",
    "train_data_enc.select(['StreamingMovies','StreamingMovies_enc']).show(15)\n",
    "print()\n",
    "\n",
    "# Data formated for xgboost:\n",
    "# train_data_enc.show(5,False)\n",
    "train_data_enc_vec.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T23:01:43.834226Z",
     "iopub.status.busy": "2020-11-23T23:01:43.833226Z",
     "iopub.status.idle": "2020-11-23T23:02:21.402301Z",
     "shell.execute_reply": "2020-11-23T23:02:21.401306Z",
     "shell.execute_reply.started": "2020-11-23T23:01:43.834226Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data took: 20.1425 seconds\n",
      "One hot encoding and dtype conversion took: 15.6191 seconds\n",
      "Converting pandas to spark dataframe took: 1.4377 seconds\n",
      "\n",
      "Total number of columns after encoding: 41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[gender: tinyint, SeniorCitizen: bigint, Partner: tinyint, Dependents: tinyint, tenure: bigint, PhoneService: tinyint, PaperlessBilling: tinyint, MonthlyCharges: double, TotalCharges: double, Churn: tinyint, InternetService_DSL: tinyint, InternetService_Fiber optic: tinyint, InternetService_No: tinyint, StreamingMovies_No: tinyint, StreamingMovies_No internet service: tinyint, StreamingMovies_Yes: tinyint, PaymentMethod_Bank transfer (automatic): tinyint, PaymentMethod_Credit card (automatic): tinyint, PaymentMethod_Electronic check: tinyint, PaymentMethod_Mailed check: tinyint, Contract_Month-to-month: tinyint, Contract_One year: tinyint, Contract_Two year: tinyint, TechSupport_No: tinyint, TechSupport_No internet service: tinyint, TechSupport_Yes: tinyint, OnlineBackup_No: tinyint, OnlineBackup_No internet service: tinyint, OnlineBackup_Yes: tinyint, DeviceProtection_No: tinyint, DeviceProtection_No internet service: tinyint, DeviceProtection_Yes: tinyint, MultipleLines_No: tinyint, MultipleLines_No phone service: tinyint, MultipleLines_Yes: tinyint, OnlineSecurity_No: tinyint, OnlineSecurity_No internet service: tinyint, OnlineSecurity_Yes: tinyint, StreamingTV_No: tinyint, StreamingTV_No internet service: tinyint, StreamingTV_Yes: tinyint]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pd_ohe_spark(filename, ohe_cols, binary_cols, label_col, drop_cols=None, verbose=False):\n",
    "    \"\"\"\n",
    "    Use pandas to one-hot-encode categorical data, then convert results to a spark dataframe using pyarrow.\n",
    "    Make sure: spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"True\").\n",
    "\n",
    "    filename: str\n",
    "        Fully specified file path.\n",
    "    ohe_cols: list\n",
    "        List of strings with categorical columns to be one-hot-encoded.\n",
    "    binary_cols: list\n",
    "        List of strings with binary categorical columns to encode. \n",
    "    label_col: str\n",
    "        Name of label/target column (i.e., y). Encoded as numeric.\n",
    "    drop_cols: list\n",
    "        List of strings with columns to drop.\n",
    "    \"\"\"\n",
    "\n",
    "    file_type = filename.split('.')[-1]\n",
    "    \n",
    "    tic = time.perf_counter()\n",
    "    if file_type == 'csv':\n",
    "        data_pd = pd.read_csv(filename)\n",
    "    elif file_type == 'parquet':\n",
    "        # Use pyarrow to load parquet data directly. Pandas very slow at loading decimal encoded parquet. Spark appears to read decimal encoded parquet properly. \n",
    "        data_pa = pq.read_table(data_loc)\n",
    "        pa_schema = data_pa.schema\n",
    "        \n",
    "        # Convert pa.decimal128 to pa.float64:\n",
    "        updated_schema = pa.schema([pa.field(dd.name, pa.float64()) if pa.types.is_decimal(dd.type) else dd for dd in pa_schema])\n",
    "\n",
    "        # Convert arrow table to pandas dataframe:\n",
    "        data_pd = data_pa.cast(updated_schema).to_pandas()\n",
    "    else:\n",
    "        raise TypeError('File type not recognized. Input file must be csv or parquet.')\n",
    "    \n",
    "    if drop_cols != None:\n",
    "        data_pd = data_pd.drop(columns=drop_cols)\n",
    "    \n",
    "    toc_load_data = time.perf_counter() - tic\n",
    "    \n",
    "    # One hot encoding:\n",
    "    tic = time.perf_counter()\n",
    "    # Use drop_fist=True to get k-1 dummies out of k categorical levels by removing the first level. (as done in sparkML OneHotEncoder)\n",
    "    data_pd_enc = pd.get_dummies(data_pd, columns=[cc for cc in ohe_cols if cc != label_col])\n",
    "\n",
    "    # Upconvert numeric datatype to supported arrow format. Unsupported formats: uint8\n",
    "    pd_dtypes = pd.DataFrame(data_pd_enc.dtypes, columns=['dtype'])\n",
    "\n",
    "    # TODO: write fnc for casting to smallest compatible dtype between spark & arrow.\n",
    "    # Data type conversion required compatibility for pandas => arrow \n",
    "    uint_cols = list(data_pd_enc.select_dtypes('uint8').columns)\n",
    "    data_pd_enc[uint_cols] = data_pd_enc[uint_cols].astype('int8')\n",
    "    \n",
    "    # Encode binary categorical columns (e.g., [False, True] or [No, Yes] => [0, 1]):\n",
    "    for cc in binary_cols:\n",
    "        data_pd_enc[cc] = data_pd_enc[cc].astype('category')\n",
    "        data_pd_enc[cc] = data_pd_enc[cc].cat.codes\n",
    "    \n",
    "#     # Encode label column if categorical:\n",
    "#     if pd.api.types.is_object_dtype(data_pd_enc[label_col].dtypes):\n",
    "#         data_pd_enc[label_col] = data_pd_enc[label_col].astype('category')\n",
    "#         data_pd_enc[label_col+'_index'] = data_pd_enc[label_col].cat.codes\n",
    "#         data_pd_enc = data_pd_enc.drop(columns=[label_col])\n",
    "    \n",
    "    toc_ohe = time.perf_counter() - tic\n",
    "    \n",
    "    # Create spark dataframe of encoded data. Make sure that spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"True\")\n",
    "    tic = time.perf_counter()\n",
    "    data_sp_enc = spark.createDataFrame(data_pd_enc)\n",
    "    toc_spark_df = time.perf_counter() - tic\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Loading data took: {toc_load_data:0.4f} seconds')\n",
    "        print(f'One hot encoding and dtype conversion took: {toc_ohe:0.4f} seconds')\n",
    "        print(f'Converting pandas to spark dataframe took: {toc_spark_df:0.4f} seconds')\n",
    "    return(data_sp_enc)\n",
    "\n",
    "# Use pandas to encode X, then pyarrow to convert X into a spark dataframe:\n",
    "data_sp = pd_ohe_spark(data_loc, ohe_cols, binary_cols, 'Churn', ignore_cols, verbose=True)\n",
    "\n",
    "print()\n",
    "print('Total number of columns after encoding:', len(data_sp.columns))\n",
    "\n",
    "data_sp_vec = (VectorAssembler()\n",
    "        .setInputCols([cc for cc in data_sp.columns if cc != label])\n",
    "        .setOutputCol('features')\n",
    "#         .setHandleInvalid('keep') # How to handle NaN's\n",
    "        .transform(data_sp)\n",
    "        .withColumn('features', sparse_to_vector_udf('features'))\n",
    "        .select(col('features'), col('Churn')))\n",
    "\n",
    "data_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T23:02:21.403317Z",
     "iopub.status.busy": "2020-11-23T23:02:21.403317Z",
     "iopub.status.idle": "2020-11-23T23:02:23.147320Z",
     "shell.execute_reply": "2020-11-23T23:02:23.146336Z",
     "shell.execute_reply.started": "2020-11-23T23:02:21.403317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                                                                                                    |Churn|\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|[0.0,0.0,1.0,0.0,1.0,0.0,1.0,34.7,34.7,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0]     |0    |\n",
      "|[1.0,0.0,0.0,0.0,35.0,1.0,0.0,53.47,1810.75,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0]|0    |\n",
      "|[1.0,0.0,0.0,0.0,2.0,1.0,1.0,51.01,102.01,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0]  |1    |\n",
      "|[1.0,0.0,0.0,0.0,50.0,0.0,0.0,37.76,1830.39,0.0,1.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0]|0    |\n",
      "|[0.0,0.0,0.0,0.0,2.0,1.0,1.0,76.82,153.64,1.0,0.0,1.0,0.0,1.0,0.0,0.0,0.0,0.0,1.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0,1.0,0.0,0.0]  |1    |\n",
      "+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_sp_vec.show(5,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-23T23:02:23.152331Z",
     "iopub.status.busy": "2020-11-23T23:02:23.152331Z",
     "iopub.status.idle": "2020-11-23T23:02:23.161327Z",
     "shell.execute_reply": "2020-11-23T23:02:23.159324Z",
     "shell.execute_reply.started": "2020-11-23T23:02:23.152331Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (<ipython-input-8-6aaf1f276005>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-6aaf1f276005>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    break\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5c510c0f-991c-4759-8ea1-ae5169bc1d70",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.status.busy": "2020-11-23T23:02:23.163324Z",
     "iopub.status.idle": "2020-11-23T23:02:23.164324Z"
    }
   },
   "outputs": [],
   "source": [
    "params = { \n",
    "    'seed': 0,\n",
    "    'treeMethod': 'hist',\n",
    "    'maxDepth': 10,\n",
    "    'numRound': 10,\n",
    "    # 'numWorkers': 1,\n",
    "    # 'nthread': 12,\n",
    "    # 'verbosity': 3\n",
    "}\n",
    "\n",
    "# Use xgboost-4j:\n",
    "classifier = xgb.XGBoostClassifier(**params).setLabelCol(label).setFeaturesCol('features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "57a82b1e-676d-4315-994f-adb064001ab3",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.status.busy": "2020-11-23T23:02:23.166324Z",
     "iopub.status.idle": "2020-11-23T23:02:23.166324Z"
    }
   },
   "outputs": [],
   "source": [
    "def with_benchmark(phrase, action):\n",
    "    start = time()\n",
    "    result = action()\n",
    "    end = time()\n",
    "    print('{} takes {} seconds'.format(phrase, round(end - start, 2)))\n",
    "    return result\n",
    "\n",
    "loaded_model = with_benchmark('Training', lambda: classifier.fit(train_data_enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "26cbe3dc-1a97-4ca2-b1cd-78d67e8226a3",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.status.busy": "2020-11-23T23:02:23.168325Z",
     "iopub.status.idle": "2020-11-23T23:02:23.169323Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply model on all data:\n",
    "def transform():\n",
    "    result = loaded_model.transform(train_data_enc).cache()\n",
    "    result.foreachPartition(lambda _: None)\n",
    "    return result\n",
    "\n",
    "result = with_benchmark('Transformation', transform)\n",
    "\n",
    "result.select('features', label).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a0e57c2d-26af-44f4-a2a8-9bcbe3f39229",
     "showTitle": false,
     "title": ""
    },
    "execution": {
     "iopub.status.busy": "2020-11-23T23:02:23.170323Z",
     "iopub.status.idle": "2020-11-23T23:02:23.171327Z"
    }
   },
   "outputs": [],
   "source": [
    "# Benchmark scoring/evaluation:\n",
    "metric_eval = with_benchmark(\n",
    "    'Evaluation',\n",
    "    lambda: BinaryClassificationEvaluator().setMetricName('areaUnderROC').setLabelCol(label).evaluate(result))\n",
    "\n",
    "print('AUC is ' + str(metric_eval))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookName": "xgboost-telco-data-cpu",
   "notebookOrigID": 2038968934101903,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
